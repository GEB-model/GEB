# Snakemake SLURM profile for GEB large-scale cluster execution
# This profile configures Snakemake to submit jobs to SLURM with appropriate resources

# Basic SLURM configuration  
# Simplified cluster command - logs go only to model directories
cluster: "sbatch --parsable --job-name={rule}_{wildcards.cluster} --account={resources.slurm_account} --ntasks=1 --cpus-per-task={resources.cpus} --mem={resources.mem_mb}M --time=8-00:00:00 {resources.partition_arg}"

# Configuration overrides
config:
  LARGE_SCALE_DIR: "/scistor/ivm/tbr910/GEB/models/large_scale"
  CLUSTER_PREFIX: "Europe"
  EVALUATION_METHODS: "plot_discharge,evaluate_discharge"

# Job management
jobs: 6
max-jobs-per-second: 2
max-status-checks-per-second: 0.5

# Partition-specific job limits
group-components:
  defq_empty: 2     # Max 2 concurrent jobs on defq (no partition arg)
  ivm_part: 2       # Max 2 concurrent jobs on ivm partition
  ivm-fat_part: 2   # Max 2 concurrent jobs on ivm-fat partition

# Resource defaults (will be overridden by rule-specific resources)
# Keep coordinator job lightweight - only cluster rules need large memory
# Use defq for coordinator (no partition specified), cluster rules will override with specific partitions
default-resources:
  - cpus=2
  - mem_mb=4000    # 4GB for lightweight coordinator job
  - runtime=11520  # 8 days in minutes (8 * 24 * 60 = 11520)
  - slurm_partition=defq
  - slurm_account=ivm
  - partition_arg=""  # Empty for defq (default queue), rules will override

# Rerun incomplete jobs and keep going on failures
rerun-incomplete: true
keep-going: true
printshellcmds: true

# Latency wait (important for network filesystems)
latency-wait: 60

# Use conda for environment management if needed
use-conda: false
use-singularity: false
